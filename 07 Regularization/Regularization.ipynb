{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "# silence future deprecation warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Regularization for Linear Regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although linear regression is a linear machine learning method, you can have nonlinear dependencies if you transform some of the independent variables by a nonlinear function. By doing this, you can improve the fit of your method. Let us demonstrate this on a house price dataset from [Kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction). Note that this dataset is not identical with one you used in the linear regression exercise, since the this dataset is too small and would cause unreliable evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"kc_house_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have a simple linear regression problem with only one independent variable. Thus, we only keep *price* and *sqft_living*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"price\",\"sqft_living\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data\n",
    "Let us normalize the data by using *min-max normalization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "test = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[[\"sqft_living\"]]\n",
    "y_train = train[[\"price\"]]\n",
    "\n",
    "X_test = test[[\"sqft_living\"]]\n",
    "y_test = test[[\"price\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias term\n",
    "To account for the bias term, we add a column containing only ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"bias\"] = 1\n",
    "X_test[\"bias\"] = 1\n",
    "\n",
    "# Force order\n",
    "X_train = X_train[[\"bias\", \"sqft_living\"]]\n",
    "X_test = X_test[[\"bias\", \"sqft_living\"]]\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression model\n",
    "Define a linear regression function to estimate the parameters $\\theta$ based on the normal equation:\n",
    "  \n",
    "  $\\Theta:=(X^{\\top}X)^{-1}(X^{\\top}y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def fit(X, y):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def fit(X, y):\n",
    "    thetas = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to check your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = fit(X_train, y_train)\n",
    "\n",
    "expected_thetas = np.array([[7.39560812e-05], [4.94185750e-01]])\n",
    "np.testing.assert_array_almost_equal(thetas, expected_thetas, decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict prices\n",
    "Using $X$ and the estimated $\\theta$, predict the house prices on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def predict(X, thetas):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def predict(X, thetas):\n",
    "    y_pred = np.dot(X, thetas)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(X_train, thetas)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions\n",
    "Let us plot house prices and predicted house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_line(X, thetas, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    deg = len(thetas)-1\n",
    "    poly = PolynomialFeatures(deg)\n",
    "    \n",
    "    xs = np.arange(X.min(), X.max()+0.1, 0.01).reshape(-1,1)\n",
    "    x = poly.fit_transform(xs)\n",
    "    y_pred = np.dot(x, thetas)\n",
    "    \n",
    "    ax.plot(xs, y_pred, color=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_train[\"sqft_living\"].values, y_train.values, \"bo\", markersize=1)\n",
    "plot_regression_line(X_train[\"sqft_living\"].values, thetas, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate model performance\n",
    "Now let's check how good our model performs by calculating the $R^2$ score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# r2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "y_pred_test = predict(X_test, thetas)\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"R2: \",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to improve the fit by adding $x^2$ as additional independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_deg2 = X_train.copy()\n",
    "X_train_deg2[\"sqft_living^2\"] = X_train_deg2[\"sqft_living\"] * X_train_deg2[\"sqft_living\"]\n",
    "\n",
    "X_test_deg2 = X_test.copy()\n",
    "X_test_deg2[\"sqft_living^2\"] = X_test_deg2[\"sqft_living\"] * X_test_deg2[\"sqft_living\"]\n",
    "X_test_deg2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model with the additonal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas_deg2 = fit(X_train_deg2, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# r2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "y_pred_test = predict(X_test_deg2, thetas_deg2)\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"R2: \",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, by adding $x^2$ as additional independent variable we could slightly improve our performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try if we can further improve our performance by adding more polynomial features. To generate our polynomial features we will use the Scikit-Learn function [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(poly_deg =(1,18,1))\n",
    "def f(poly_deg=1):\n",
    "    poly = PolynomialFeatures(poly_deg)\n",
    "    X_train_deg = poly.fit_transform(X_train[\"sqft_living\"].values.reshape(-1,1))\n",
    "    X_test_deg = poly.fit_transform(X_test[\"sqft_living\"].values.reshape(-1,1))\n",
    "\n",
    "    thetas_deg = fit(X_train_deg, y_train)\n",
    "    \n",
    "    y_pred_test = predict(X_test_deg, thetas_deg)\n",
    "    y_pred_train = predict(X_train_deg, thetas_deg)\n",
    "    \n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    print(\"R2 Train {0:.5f}\".format(r2_train))\n",
    "    print(\"R2 Test {0:.5f}\".format(r2_test))\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(20,10))\n",
    "    ax0.set_title(\"Training data - polynomial degree {}\".format(poly_deg))\n",
    "    ax0.plot(X_train[\"sqft_living\"], y_train[\"price\"], \"bo\", markersize=1)\n",
    "    plot_regression_line(X_train_deg, thetas_deg, ax0)\n",
    "    \n",
    "    ax1.set_title(\"Test data - polynomial degree {}\".format(poly_deg))\n",
    "    ax1.plot(X_test[\"sqft_living\"], y_test[\"price\"], \"bo\", markersize=1)\n",
    "    plot_regression_line(X_test_deg, thetas_deg, ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you recognize when you increase the polynomial degree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the question on ILIAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of overfitting can be reduced by regularization. Implement the regularized version of linear regression: $\\Theta:=(X^{\\top}X+\\lambda \\begin{bmatrix}\n",
    "    0  & 0 &\\ldots&0 \\\\\n",
    "    0 & 1 & \\\\\n",
    "    \\ldots & & \\ddots & \\\\\n",
    "    0& & & 1\n",
    "  \\end{bmatrix} )^{-1}(X^{\\top}y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def fit_reg(X, y, lam):\n",
    "    # START YOUR CODE\n",
    "\n",
    "    # END YOUR CODE\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def fit_reg(X, y, lam):\n",
    "    Xt = np.transpose(X)\n",
    "    XtX = np.dot(Xt,X)\n",
    "    XtX = XtX + (lam * np.identity(XtX.shape[0]))\n",
    "    XtXm1 = np.linalg.inv(XtX)\n",
    "    Xty = np.dot(Xt,y)\n",
    "    thetas = np.dot(XtXm1,Xty)\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your implementation by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_thetas = np.array([[0.0017878 ], [0.48483375]])\n",
    "actual_thetas = fit_reg(X_train, y_train, lam=2)\n",
    "\n",
    "np.testing.assert_array_almost_equal(expected_thetas, actual_thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  plot the graph using the regularized parameter vectors. As you can see, the effect of overfitting is strongly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(poly_deg = (0,12,1), lam=(0,100,1))\n",
    "def f(poly_deg=1, lam=4):\n",
    "    poly = PolynomialFeatures(poly_deg)\n",
    "    X_train_deg = poly.fit_transform(X_train[\"sqft_living\"].values.reshape(-1,1))\n",
    "    X_test_deg = poly.fit_transform(X_test[\"sqft_living\"].values.reshape(-1,1))\n",
    "\n",
    "    thetas_deg = fit_reg(X_train_deg, y_train, lam=lam)\n",
    "    \n",
    "    y_pred_test = predict(X_test_deg, thetas_deg)\n",
    "    y_pred_train = predict(X_train_deg, thetas_deg)\n",
    "    \n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    print(\"R2 Train\", r2_train)\n",
    "    print(\"R2 Test\", r2_test)\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(20,10))\n",
    "    ax0.set_title(\"Training data - polynomial degree {}\".format(poly_deg))\n",
    "    ax0.plot(X_train[\"sqft_living\"], y_train[\"price\"], \"bo\", markersize=1)\n",
    "    plot_regression_line(X_train_deg, thetas_deg, ax0)\n",
    "    \n",
    "    ax0.set_title(\"Test data - polynomial degree {}\".format(poly_deg))\n",
    "    ax1.plot(X_test[\"sqft_living\"], y_test[\"price\"], \"bo\", markersize=1)\n",
    "    plot_regression_line(X_test_deg, thetas_deg, ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best configuration of **polynomial degree** and $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>PLEASE REPLACE TEXT WITH YOUR CONFIGURATION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization to help with numerical issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another benefit of regularization is that it can help in case of numerical issues. Let us consider our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"kc_house_data.csv\")\n",
    "df = df[[\"price\",\"sqft_living\",\"bedrooms\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)\n",
    "test = pd.DataFrame(scaler.fit_transform(test), columns=test.columns, index=test.index)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the feature matrix $X^{\\top}X$ singular, we just add  another independent variable (Size2) to X\n",
    "that amounts to just twice the Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"sqft_living2\"] = 2 * train[\"sqft_living\"]\n",
    "train[\"bias\"] = 1\n",
    "\n",
    "test[\"sqft_living2\"]=2 * test[\"sqft_living\"]\n",
    "test[\"bias\"] = 1\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[[\"bias\", \"sqft_living\", \"bedrooms\", \"sqft_living2\"]]\n",
    "y_train = train[[\"price\"]]\n",
    "\n",
    "X_test = test[[\"bias\", \"sqft_living\", \"bedrooms\", \"sqft_living2\"]]\n",
    "y_test = test[[\"price\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the linear regression fails, since $X^{\\top}X$ is not invertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two possiblities to tackle this issue, the first one is to use the pseudoinverse instead of the inverse\n",
    "and the second one is using regularization. \n",
    "\n",
    "> Try out both. \n",
    "\n",
    "*Hint*: For conducting linear regression with the pseudoinverse, you have to slightly modify the linear_regression method given further above. \n",
    "The numpy function [np.linalg.pinv](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) becomes handy for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def fit_pseudoinverse(X,y):\n",
    "    # START YOUR CODE\n",
    "    \n",
    "    # END YOUR CODE\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "def fit_pseudoinverse(X, y):\n",
    "    thetas = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code to check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "thetas_pseudo_inverse = fit_pseudoinverse(X_train, y_train)\n",
    "print (\"thetas obtained by linear regression with pseudoinverse:\\n\",thetas_pseudo_inverse)\n",
    "\n",
    "expected_thetas_pseudo_inverse = np.array([\n",
    "    [ 0.02902459],\n",
    "    [ 0.11220321],\n",
    "    [-0.12253607],\n",
    "    [ 0.22440641]])\n",
    "\n",
    "np.testing.assert_array_almost_equal(thetas_pseudo_inverse, expected_thetas_pseudo_inverse, decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas_regularization = fit_reg(X_train, y_train,lam=1)\n",
    "print (\"thetas obtained by linear regression with regularization:\\n\",thetas_regularization)\n",
    "\n",
    "expected_thetas_regularization = np.array([\n",
    "    [ 0.02842384],\n",
    "    [ 0.11163472],\n",
    "    [-0.11920766],\n",
    "    [ 0.22326945]])\n",
    "\n",
    "np.testing.assert_array_almost_equal(thetas_regularization, expected_thetas_regularization, decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularization for Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us apply regularization to a larger dataset using sklearn, for which we reuse the eczema dataset from the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"skin_disease.csv\")\n",
    "df.drop('t0', axis=1, inplace=True)\n",
    "df.drop('t1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again split the data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"class\"])\n",
    "y = df[[\"class\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the data using the RobustScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Logistic Regression model\n",
    "> Train a Logistic Regression model by using the Scikit-Learn class [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). And calculate the accuracy and the f1 score on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# START YOU CODE\n",
    "\n",
    "\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Accuracy\", accuracy)\n",
    "print(\"F1 Score\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularize our Logistic Regression model\n",
    "> For the logistic regression model we can also apply L2 regularization. Try systematically all L2 regularization parameters in the interval [0,2] with step size 0.1, a technique also known as grid search. Calculate for each iteration the accuracy and the f1 score.\n",
    "\n",
    "Hint: Take a look at the documentation of the [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class and specify the correct **regularization type** with the parameter **penalty**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "f1_scores = []\n",
    "C_values = np.arange(0.1, 2.1, 0.1)\n",
    "for c in tqdm(C_values):\n",
    "    # START YOUR CODE\n",
    "    #model = LogisticRegression(...)\n",
    "    \n",
    "    \n",
    "    pass\n",
    "    # END YOUR CODE\n",
    "    \n",
    "max_acc_idx = np.argmax(accuracies)\n",
    "print(\"Best accuracy: {0:.4f} with C-value: {1:.2f}\".format(accuracies[max_acc_idx], C_values[max_acc_idx]))\n",
    "print(\"Best f1_score: {0:.4f} with C-value: {1:.2f}\".format(f1_scores[max_acc_idx], C_values[max_acc_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "f1_scores = []\n",
    "C_values = np.arange(0.1, 2.1, 0.1)\n",
    "for c in tqdm(C_values):\n",
    "    model = LogisticRegression(penalty=\"l2\", C=c)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    f1 = f1_score(y_test, y_pred_test)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "max_acc_idx = np.argmax(accuracies)\n",
    "print(\"Best accuracy: {0:.4f} with C-value: {1:.2f}\".format(accuracies[max_acc_idx], C_values[max_acc_idx]))\n",
    "print(\"Best f1_score: {0:.4f} with C-value: {1:.2f}\".format(f1_scores[max_acc_idx], C_values[max_acc_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Search with Scikit-Learn\n",
    "This can be also done directly with Scikit-Learn. The following cell performs a grid search using a 5-fold cross validation.\n",
    "\n",
    "<font color='red'>This can take a while!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=\"l2\")\n",
    "params = {\n",
    "    'C': np.arange(0.1, 2.1, 0.1)\n",
    "}\n",
    "grid_search = GridSearchCV(model, params, cv=5, scoring=\"f1\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best params\", grid_search.best_params_)\n",
    "print(\"Best f1 score\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best C value is not the same as we got when we performed our grid search manually. The reason is that we did not use cross validation but tuned our hyperparameters on the test set. This should not be done because then we overfit our model on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final remark, in this notebook we used for simplicity skin pixels from the same image for training and testing.\n",
    "In real world scenarios, the test and training data set should originate from different images, \n",
    "since skin pixels from the same image are highly correlated and your evaluation measure values would therefore \n",
    "probably be too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "> Now finish the ILIAS quiz to **Regularization**."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.2",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
